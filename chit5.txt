# Import necessary libraries
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.applications import MobileNetV2

# Step 1: Load the CIFAR-10 dataset
(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize the images to [0, 1] range for better performance during training
train_images, test_images = train_images / 255.0, test_images / 255.0

# Display a few images from the dataset along with their labels
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

plt.figure(figsize=(10, 10))
for i in range(10):
    plt.subplot(5, 5, i + 1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i])  # Show the image
    plt.xlabel(class_names[train_labels[i][0]])  # Show the label
plt.show()

# Step 2: Load a pre-trained MobileNetV2 model without the top classifier
base_model = MobileNetV2(input_shape=(32, 32, 3), include_top=False, weights='imagenet')

# Freeze the base model to retain learned features and prevent them from being updated during initial training
base_model.trainable = False

# Step 3: Create a custom classifier on top of the base model
model = models.Sequential([
    base_model,  # Add the pre-trained base model
    layers.Flatten(),  # Flatten the output from the base model
    layers.Dense(64, activation='relu'),  # Add a fully connected layer
    layers.Dense(10)  # Output layer with 10 classes for CIFAR-10
])

# Step 4: Compile the model with appropriate loss and metrics
model.compile(optimizer='adam',  # Use Adam optimizer
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Loss function
              metrics=['accuracy'])  # Metric for evaluation

# Step 5: Train the model on the training data
epochs = 10  # Number of epochs for initial training
history = model.fit(train_images, train_labels, epochs=epochs, validation_data=(test_images, test_labels))

# Step 6: Fine-tune the model by unfreezing some layers of the base model
base_model.trainable = True  # Unfreeze the base model

# Optionally, unfreeze the last few layers for fine-tuning
for layer in base_model.layers[:-10]:  # Unfreeze all but the last 10 layers
    layer.trainable = False  # Freeze these layers

# Recompile the model after changing layer trainability to apply the changes
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),  # Use a smaller learning rate for fine-tuning
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Continue training the model with fine-tuning
fine_tune_epochs = 10  # Number of epochs for fine-tuning
history_fine = model.fit(train_images, train_labels, epochs=fine_tune_epochs, validation_data=(test_images, test_labels))

# Optional: Plot training and validation accuracy and loss
plt.figure(figsize=(12, 4))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')  # Training accuracy
plt.plot(history.history['val_accuracy'], label='Val Accuracy')  # Validation accuracy
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')  # Training loss
plt.plot(history.history['val_loss'], label='Val Loss')  # Validation loss
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()